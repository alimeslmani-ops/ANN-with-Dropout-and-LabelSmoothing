{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4374d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed2bbd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random initialization of ANN weights and biases\n",
    "\n",
    "def create_ANN():\n",
    "    '''\n",
    "    initialize a feedforward ANN with the architecture:\n",
    "    2 inputs -> 3 hidden neurons -> 2 outputs\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    n.seed(42)\n",
    "\n",
    "    # Hidden layer: 2 inputs -> 3 neurons\n",
    "    # W1 shape: (3,2) ; b1 shape: (3,1)\n",
    "    W1 = np.random.randn(3,2) * 0.01 # small random weights\n",
    "    b1 = np.zeros((3,1)) # zero biases\n",
    "\n",
    "    # Output layer: 3 hidden -> 2 outputs\n",
    "    # W2 shape: (2,3) ; b1 shape: (2,1)\n",
    "    W2 = np.random.randn(2,3) * 0.01\n",
    "    b2 = np.zeros((2,1))\n",
    "\n",
    "    # store the parameters\n",
    "    parameters = {\n",
    "        'W1' : W1,\n",
    "        'b1' : b1,\n",
    "        'W2' : W2,\n",
    "        'b2' : b2\n",
    "    }\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f876a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch: is a small amount of training data used for one gradient update\n",
    "# so this function only selects data in batches\n",
    "# it 1- shuffles the dataset\n",
    "#    2- Yields mini-batches of (X_batch , Y_batch)\n",
    "\n",
    "def mini_batch(X , Y , batch_size):\n",
    "    # ensure inputs are numpy arrays\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    N = X.shape[0] # number of samples\n",
    "    assert N == Y.shape[0], ' X and Y must have the number of samples '\n",
    "\n",
    "    # 1- Create shuffeled indicies of X and Y in the same order\n",
    "    indicies = list(range(N))\n",
    "    n.shuffle(indicies)\n",
    "\n",
    "    indicies = np.array(indicies)\n",
    "\n",
    "    # 2- Yield batches one at a time\n",
    "    for i in range(0 , N , batch_size):\n",
    "        batch_indicies = indicies[i : i + batch_size]\n",
    "        X_batch = X[batch_indicies]\n",
    "        Y_batch = Y[batch_indicies]\n",
    "        yield (X_batch , Y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------(Activation Function for continuous labels [Regression])------\n",
    "def sigmoid(z):\n",
    "    pass\n",
    "\n",
    "def derivative_sigmoid(z_prime):\n",
    "    pass\n",
    "#-----------------------------------------------------\n",
    "\n",
    "#------(Activation Function for hot encoded labels [Classification])------\n",
    "def softmax(z):\n",
    "    pass\n",
    "\n",
    "def derivative_softmax(z_prime):\n",
    "    pass\n",
    "#--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03158d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X_batchh , param , keep_prob = 0.5 , training = True , smoothing = False):\n",
    "    W1 , b1 = param['W1'] , param['b1']\n",
    "    W2 , b2 = param['W2'] , param['b2']\n",
    "\n",
    "    m = X_batchh.shape[0]\n",
    "    X_T = X_batchh.T\n",
    "\n",
    "    #layer 1: input -> hidden\n",
    "    z1 = np.dot(W1 , X_T) + b1\n",
    "    a1 = np.maximum(0 , z1) # ReLU activation\n",
    "\n",
    "    #-----------(Inverted Dropout)-------------#\n",
    "        # inverted dropout is applied only during training\n",
    "    if training:\n",
    "        # create dropout mask: 1 -> keep , 0 -> drop\n",
    "        dropout_mask = (np.random.rand(*a1.shape) > keep_prob).astype(float)\n",
    "        # scale surviving neurons by 1/(1 - keep_prob) -> 'inverted dropout'\n",
    "        a1 = a1 * dropout_mask / (1.0 - keep_prob)\n",
    "    else:\n",
    "        dropout_mask = None # not used in inference (prediction phase)\n",
    "    #-------------------------------------------#\n",
    "\n",
    "    #layer 2: hidden -> output\n",
    "    z2 = np.dot(W2 , a1) + b2\n",
    "    a2 = np.maximum(0 , z2) # ReLU activation\n",
    "\n",
    "    #-----------(Label Smoothing)-------------#\n",
    "    if smoothing != False:\n",
    "        epsilon = 0.1\n",
    "        num_classes = a2.shape[0]\n",
    "        a2 = a2 * (1 - epsilon) + (epsilon / num_classes)\n",
    "    #-----------------------------------------#\n",
    "\n",
    "\n",
    "    cache = {\n",
    "        'X_T' : X_T,\n",
    "        'z1': z1,\n",
    "        'a1_before_dropout': np.maximum(0 , z1), #needed for backprop without scaling\n",
    "        'a1' : a1,\n",
    "        'dropout_mask' : dropout_mask,\n",
    "        'z2' : z2,\n",
    "        'a2' : a2,\n",
    "        'keep_prob' : keep_prob,\n",
    "        'training' : training\n",
    "    }\n",
    "\n",
    "    return a2 , cache\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(a2 , Y_batch):\n",
    "    #Cross-entropy Loss because the outputs are hot encoded\n",
    "    pass\n",
    "\n",
    "def compute_loss(a2 , Y_batch):\n",
    "    # Compute MSE loss (Mean Sequared Error)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(a2 , Y_batch , cache , param):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b7e20",
   "metadata": {},
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bbc306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random training data\n",
    "# inputs\n",
    "X_ = np.random.randn(100 , 2) # 100 samples , 2 features\n",
    "# outputs\n",
    "Y_ = np.random.randint(0 , 2 , size=(100 , 2)) # 100 samples , 2 classes (one-hot encoded)\n",
    "\n",
    "# learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Dropout keep-probability\n",
    "#keep_prob = 0.5\n",
    "\n",
    "# label smoothing parameter (epsilon)\n",
    "# label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944176d4",
   "metadata": {},
   "source": [
    "Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a794ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_ANN()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for X_batch , Y_batch in mini_batch(X_ , Y_ , batch_size = 32):\n",
    "        # print(\"X_batch shape:\", X_batch.shape)\n",
    "        # print(\"Y_batch shape:\", Y_batch.shape)\n",
    "        # pass\n",
    "\n",
    "        #-----(feed forward with inverted dropout and label smoothing)-----\n",
    "        a_output , cache = feed_forward(X_batch , model , keep_prob = 0.5 , training = True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
