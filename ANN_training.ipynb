{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4374d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349c1e5",
   "metadata": {},
   "source": [
    "Create the Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bbd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random initialization of ANN weights and biases\n",
    "\n",
    "def create_ANN():\n",
    "    '''\n",
    "    initialize a ANN with the architecture:\n",
    "    2 inputs -> 3 hidden neurons -> 2 outputs\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    n.seed(42)\n",
    "\n",
    "    # Hidden layer: 2 inputs -> 3 neurons\n",
    "    # W1 shape: (3,2) ; b1 shape: (3,1)\n",
    "    W1 = np.random.randn(3,2) * 0.01 # small random weights\n",
    "    b1 = np.zeros((3,1)) # zero biases\n",
    "\n",
    "    # Output layer: 3 hidden -> 2 outputs\n",
    "    # W2 shape: (2,3) ; b1 shape: (2,1)\n",
    "    W2 = np.random.randn(2,3) * 0.01\n",
    "    b2 = np.zeros((2,1))\n",
    "\n",
    "    # store the parameters\n",
    "    parameters = {\n",
    "        'W1' : W1,\n",
    "        'b1' : b1,\n",
    "        'W2' : W2,\n",
    "        'b2' : b2\n",
    "    }\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0502b",
   "metadata": {},
   "source": [
    "Split the Data in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f876a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch: is a small amount of training data used for one gradient update\n",
    "# so this function only selects data in batches\n",
    "# it 1- shuffles the dataset\n",
    "#    2- Yields mini-batches of (X_batch , Y_batch)\n",
    "\n",
    "def mini_batch(X , Y , batch_size):\n",
    "    # ensure inputs are numpy arrays\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    N = X.shape[0] # number of samples\n",
    "    assert N == Y.shape[0], ' X and Y must have the number of samples '\n",
    "\n",
    "    # 1- Create shuffeled indicies of X and Y in the same order\n",
    "    indicies = list(range(N))\n",
    "    n.shuffle(indicies)\n",
    "\n",
    "    indicies = np.array(indicies)\n",
    "\n",
    "    # 2- Yield batches one at a time\n",
    "    for i in range(0 , N , batch_size):\n",
    "        batch_indicies = indicies[i : i + batch_size]\n",
    "        X_batch = X[batch_indicies]\n",
    "        Y_batch = Y[batch_indicies]\n",
    "        yield (X_batch , Y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4314d",
   "metadata": {},
   "source": [
    "Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd0c4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------(Activation Function for continuous labels [Regression])------\n",
    "def sigmoid(z):\n",
    "    sigmoid_output = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    return sigmoid_output\n",
    "\n",
    "def derivative_sigmoid(sigmoid_output):\n",
    "    sigmoid_derivative = sigmoid_output * (1 - sigmoid_output)\n",
    "\n",
    "    return sigmoid_derivative\n",
    "#-----------------------------------------------------\n",
    "\n",
    "#------(Activation Function for hot encoded labels [Classification])------\n",
    "def softmax(z):\n",
    "    # Subtract the maximum value for numerical stability\n",
    "    # keepdims=True ensures the max is broadcasted correctly across dimensions for 2D arrays\n",
    "    x = z - np.max(z , axis = -1 , keepdims = True)\n",
    "    numerator = np.exp(x)\n",
    "    denominator = np.sum(numerator , axis = -1 , keepdims = True)\n",
    "    softmax_output = numerator / denominator\n",
    "\n",
    "    return softmax_output\n",
    "\n",
    "def derivative_softmax(softmax_output):\n",
    "    s = softmax_output.reshape(-1 , 1)\n",
    "    # The Jacobian is calculated as diag(s) - dot(s, s.T)\n",
    "    jacobian_matrix = np.diagflat(s) - np.dot(s , s.T)\n",
    "\n",
    "    return jacobian_matrix\n",
    "#--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6831de24",
   "metadata": {},
   "source": [
    "Performing Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03158d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X_batchh , param , keep_prob = 0.5 , training = True , smoothing = False):\n",
    "    W1 , b1 = param['W1'] , param['b1']\n",
    "    W2 , b2 = param['W2'] , param['b2']\n",
    "\n",
    "    m = X_batchh.shape[0]\n",
    "    X_T = X_batchh.T\n",
    "\n",
    "    #layer 1: input -> hidden\n",
    "    z1 = np.dot(W1 , X_T) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    #-----------(Inverted Dropout)-------------#\n",
    "        # inverted dropout is applied only during training\n",
    "    if training:\n",
    "        # create dropout mask: 1 -> keep , 0 -> drop\n",
    "        dropout_mask = (np.random.rand(*a1.shape) > keep_prob).astype(float)\n",
    "        # scale surviving neurons by 1/(1 - keep_prob) -> 'inverted dropout'\n",
    "        a1 = a1 * dropout_mask / (1.0 - keep_prob)\n",
    "    else:\n",
    "        dropout_mask = None # not used in inference (prediction phase)\n",
    "    #-------------------------------------------#\n",
    "\n",
    "    #layer 2: hidden -> output\n",
    "    z2 = np.dot(W2 , a1) + b2\n",
    "    a2 = softmax(z2)\n",
    "\n",
    "    #-----------(Label Smoothing)-------------#\n",
    "    if smoothing != False:\n",
    "        epsilon = 0.1\n",
    "        num_classes = a2.shape[0]\n",
    "        a2 = a2 * (1 - epsilon) + (epsilon / num_classes)\n",
    "    #-----------------------------------------#\n",
    "\n",
    "\n",
    "    cache = {\n",
    "        'X_T' : X_T,\n",
    "        'z1': z1,\n",
    "        'a1_before_dropout': sigmoid(z1), #needed for backprop without scaling\n",
    "        'a1' : a1,\n",
    "        'dropout_mask' : dropout_mask,\n",
    "        'z2' : z2,\n",
    "        'a2' : a2,\n",
    "        'keep_prob' : keep_prob,\n",
    "        'training' : training\n",
    "    }\n",
    "\n",
    "    return a2 , cache\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8a8f0",
   "metadata": {},
   "source": [
    "Loss Functions (Not Used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5a240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "def cross_entropy_loss(a2 , Y_batch):\n",
    "    #Cross-entropy Loss if the outputs are hot encoded\n",
    "    pred = np.clip(a2 , 1e-15 , 1 - 1e-15) # clip predictions to avoid log(0) or log(1-0) erros\n",
    "\n",
    "    # calculate the loss for each sample\n",
    "    loss = - np.sum(Y_batch.T * np.log(pred))\n",
    "\n",
    "    return np.mean(loss)\n",
    "#----------------------------------------\n",
    "\n",
    "#----------------------------------------\n",
    "def compute_loss(a2 , Y_batch):\n",
    "    # Compute MSE loss (Mean Sequared Error)\n",
    "    loss = np.mean(np.square(a2 - Y_batch.T)) # Y_batch.T to match dimensions\n",
    "\n",
    "    return loss\n",
    "#----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a933c",
   "metadata": {},
   "source": [
    "Performing Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7524de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(Y_batch , cache , param):\n",
    "    W2 = param['W2']\n",
    "    Y_T = Y_batch.T # (2 , m)\n",
    "    m = Y_batch.shape[1] # number of samples in the batch\n",
    "\n",
    "    # Output layer delta (error signal)\n",
    "    dz2 = cache['a2'] - Y_T\n",
    "    dW2 = (1 / m) * np.dot(dz2 , cache['a1'].T)\n",
    "    db2 = (1 / m) * np.sum(dz2 , axis = 1 , keepdims = True)\n",
    "\n",
    "    # Hidden layer delta (error signal)\n",
    "    da1 = np.dot(W2.T , dz2) # (3 , m)\n",
    "    if cache['training']:\n",
    "        # apply same mask mask during backpropagation\n",
    "        da1 = da1 * cache['dropout_mask'] / (1.0 - cache['keep_prob'])\n",
    "\n",
    "    a1_clean = cache['a1_before_dropout'] # unscaled activation\n",
    "\n",
    "    dz1 = da1 * derivative_sigmoid(a1_clean) # (3 , m)\n",
    "    dW1 = (1 / m) * np.dot(dz1 , cache['X_T'].T)\n",
    "    db1 = (1 / m) * np.sum(dz1 , axis =  1 , keepdims = True)\n",
    "    \n",
    "    gradients = {\n",
    "        'dW1' : dW1,\n",
    "        'db1' : db1,\n",
    "        'dW2' : dW2,\n",
    "        'db2' : db2\n",
    "    }\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eabf09",
   "metadata": {},
   "source": [
    "Updating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce07a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params , gradients , learning_rate = 0.1):\n",
    "    # for k, v in gradients.items():\n",
    "    #     print(k, type(v))\n",
    "\n",
    "    params['W1'] -= learning_rate * gradients['dW1']\n",
    "    params['b1'] -= learning_rate * gradients['db1']\n",
    "    params['W2'] -= learning_rate * gradients['dW2']\n",
    "    params['b2'] -= learning_rate * gradients['db2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b7e20",
   "metadata": {},
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bbc306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random training data\n",
    "# inputs\n",
    "X_ = np.random.randn(100 , 2) # 100 samples , 2 features\n",
    "# outputs\n",
    "Y_ = np.random.randint(0 , 2 , size=(100 , 2)) # 100 samples , 2 classes (one-hot encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944176d4",
   "metadata": {},
   "source": [
    "Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a794ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg Loss: 68.4261\n",
      "Epoch 10, Avg Loss: 77.3668\n",
      "Epoch 20, Avg Loss: 76.9996\n",
      "Epoch 30, Avg Loss: 75.1854\n",
      "Epoch 40, Avg Loss: 75.1825\n",
      "Epoch 50, Avg Loss: 73.3576\n",
      "Epoch 60, Avg Loss: 73.0293\n",
      "Epoch 70, Avg Loss: 73.8742\n",
      "Epoch 80, Avg Loss: 73.9762\n",
      "Epoch 90, Avg Loss: 75.1918\n"
     ]
    }
   ],
   "source": [
    "model = create_ANN()\n",
    "\n",
    "epoches = 100\n",
    "lr = 0.1 # learning_rate\n",
    "keep_prob = 0.5\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    epoch_loss = 0\n",
    "    number_of_batches = 0\n",
    "\n",
    "    for X_batch , Y_batch in mini_batch(X_ , Y_ , batch_size = 32):\n",
    "\n",
    "        #-----( feed forward with inverted dropout and label smoothing )-----\n",
    "        a_output , cache = feed_forward(X_batch , model , keep_prob , training = True , smoothing = True)\n",
    "\n",
    "        #-----( compute loss )-----\n",
    "        loss = cross_entropy_loss(a_output , Y_batch)\n",
    "        epoch_loss += loss\n",
    "        number_of_batches += 1\n",
    "\n",
    "        #-----( backpropagation with inverted dropout )-----\n",
    "        gradients = backpropagation(Y_batch , cache , model)\n",
    "\n",
    "        #-----( update parameters )-----\n",
    "        update_parameters(model , gradients , lr)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / number_of_batches\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
